{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Imagine trying to find a hidden treasure, but the path is cluttered with useless debris—every step slowing you down, every distraction keeping you from reaching your goal. This is exactly what happens when we work with language data in Natural Language Processing. The beauty of the message, the power of the sentiment, the core of the communication—all buried under a pile of irrelevant words that add no meaning, no insight, no direction.\n",
    "\n",
    "But there is hope! Just as we clear the path to reveal the treasure, so too must we cleanse our data to reveal the true essence of the language. And how do we do that? By removing the noise. By stripping away the words that are so frequent, so common, that they blur the lines of what truly matters. These words, often called stop words, are like static on a radio—always present, always there, but not the melody we seek.\n",
    "\n",
    "When we remove stop words, we aren’t just cleaning up data; we are sharpening our focus. We are transforming our understanding of language from a chaotic mess to a clear and meaningful structure. In that clarity, we find the patterns that lead to predictions, the insights that drive decisions, and the connections that build knowledge.\n",
    "\n",
    "So let us embrace the process of stop word removal, not as a mundane task, but as a powerful step toward discovering the heart of communication. Let us approach it with the excitement of uncovering hidden meaning, of distilling pure insight from raw language. Remember, it’s not just about what we remove, but about what we uncover in the process.\n",
    "\n",
    "Every time we eliminate those unnecessary words, we get closer to understanding the true message, the real story. And in that understanding, we unlock the full potential of language—the most powerful tool we have to connect, to create, and to change the world.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Karthick\n",
      "[nltk_data]     Selvam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Imagine trying to find a hidden treasure, but the path is cluttered with useless debris—every step slowing you down, every distraction keeping you from reaching your goal.',\n",
       " 'This is exactly what happens when we work with language data in Natural Language Processing.',\n",
       " 'The beauty of the message, the power of the sentiment, the core of the communication—all buried under a pile of irrelevant words that add no meaning, no insight, no direction.',\n",
       " 'But there is hope!',\n",
       " 'Just as we clear the path to reveal the treasure, so too must we cleanse our data to reveal the true essence of the language.',\n",
       " 'And how do we do that?',\n",
       " 'By removing the noise.',\n",
       " 'By stripping away the words that are so frequent, so common, that they blur the lines of what truly matters.',\n",
       " 'These words, often called stop words, are like static on a radio—always present, always there, but not the melody we seek.',\n",
       " 'When we remove stop words, we aren’t just cleaning up data; we are sharpening our focus.',\n",
       " 'We are transforming our understanding of language from a chaotic mess to a clear and meaningful structure.',\n",
       " 'In that clarity, we find the patterns that lead to predictions, the insights that drive decisions, and the connections that build knowledge.',\n",
       " 'So let us embrace the process of stop word removal, not as a mundane task, but as a powerful step toward discovering the heart of communication.',\n",
       " 'Let us approach it with the excitement of uncovering hidden meaning, of distilling pure insight from raw language.',\n",
       " 'Remember, it’s not just about what we remove, but about what we uncover in the process.',\n",
       " 'Every time we eliminate those unnecessary words, we get closer to understanding the true message, the real story.',\n",
       " 'And in that understanding, we unlock the full potential of language—the most powerful tool we have to connect, to create, and to change the world.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords And Filter And then Apply Stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imagin tri find hidden treasur , path clutter useless debris—everi step slow , everi distract keep reach goal .',\n",
       " 'thi exactli happen work languag data natur languag process .',\n",
       " 'the beauti messag , power sentiment , core communication—al buri pile irrelev word add mean , insight , direct .',\n",
       " 'but hope !',\n",
       " 'just clear path reveal treasur , must cleans data reveal true essenc languag .',\n",
       " 'and ?',\n",
       " 'by remov nois .',\n",
       " 'by strip away word frequent , common , blur line truli matter .',\n",
       " 'these word , often call stop word , like static radio—alway present , alway , melodi seek .',\n",
       " 'when remov stop word , ’ clean data ; sharpen focu .',\n",
       " 'we transform understand languag chaotic mess clear meaning structur .',\n",
       " 'in clariti , find pattern lead predict , insight drive decis , connect build knowledg .',\n",
       " 'so let us embrac process stop word remov , mundan task , power step toward discov heart commun .',\n",
       " 'let us approach excit uncov hidden mean , distil pure insight raw languag .',\n",
       " 'rememb , ’ remov , uncov process .',\n",
       " 'everi time elimin unnecessari word , get closer understand true messag , real stori .',\n",
       " 'and understand , unlock full potenti language—th power tool connect , creat , chang world .']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowballstemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords And Filter And then Apply Snowball Stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imagin tri find hidden treasur , path clutter useless debris—everi step slow , everi distract keep reach goal .',\n",
       " 'this exact happen work languag data natur languag process .',\n",
       " 'the beauti messag , power sentiment , core communication—al buri pile irrelev word add mean , insight , direct .',\n",
       " 'but hope !',\n",
       " 'just clear path reveal treasur , must cleans data reveal true essenc languag .',\n",
       " 'and ?',\n",
       " 'by remov nois .',\n",
       " 'by strip away word frequent , common , blur line truli matter .',\n",
       " 'these word , often call stop word , like static radio—alway present , alway , melodi seek .',\n",
       " 'when remov stop word , ’ clean data ; sharpen focus .',\n",
       " 'we transform understand languag chaotic mess clear meaning structur .',\n",
       " 'in clariti , find pattern lead predict , insight drive decis , connect build knowledg .',\n",
       " 'so let us embrac process stop word remov , mundan task , power step toward discov heart communic .',\n",
       " 'let us approach excit uncov hidden mean , distil pure insight raw languag .',\n",
       " 'rememb , ’ remov , uncov process .',\n",
       " 'everi time elimin unnecessari word , get closer understand true messag , real stori .',\n",
       " 'and understand , unlock full potenti language—th power tool connect , creat , chang world .']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords And Filter And then Apply Snowball Stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    #sentences[i]=sentences[i].lower()\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imagin tri find hide treasur , path clutter useless debris—everi step slow , everi distract keep reach goal .',\n",
       " 'exact happen work languag data natur languag process .',\n",
       " 'beauti messag , power sentiment , core communication—al buri pile irrelev word add mean , insight , direct .',\n",
       " 'hope !',\n",
       " 'clear path reveal treasur , must clean data reveal true essenc languag .',\n",
       " '?',\n",
       " 'remov nois .',\n",
       " 'strip away word frequent , common , blur line truli matter .',\n",
       " 'word , often call stop word , like static radio—alway present , alway , melodi seek .',\n",
       " 'remov stop word , ’ clean data ; sharpen focus .',\n",
       " 'transform understand languag chaotic mess clear mean structur .',\n",
       " 'clariti , find pattern lead predict , insight drive decis , connect build knowledg .',\n",
       " 'let us embrac process stop word remov , mundan task , power step toward discov heart communic .',\n",
       " 'let us approach excit uncov hide mean , distil pure insight raw languag .',\n",
       " 'rememb , ’ remov , uncov process .',\n",
       " 'everi time elimin unnecessari word , get closer understand true messag , real stori .',\n",
       " 'understand , unlock full potenti language—th power tool connect , creat , chang world .']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
